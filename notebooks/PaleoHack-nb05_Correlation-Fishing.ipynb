{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://github.com/LinkedEarth/Logos/raw/master/PYLEOCLIM_logo_HORZ-01.png' width=\"800\">\n",
    "\n",
    "# 5. Fishing for Correlations at Sea\n",
    "Correlation analysis, despite its simplicity and many shortcomings, remains a centerpiece of empirical analysis in many fields, particularly the paleosciences. Computing correlations is trivial enough; the difficulty lies in properly assessing their significance. Of particular importance are four considerations:\n",
    "\n",
    "1. __Persistence__, which violates the standard assumption that the data are independent (which underlies the classical test of significance implemented, e.g. in Excel).\n",
    "1. __Time irregularities__, for instance comparing two records with different time axes, possibly unevenly spaced (which standard software cannot deal with out of the box)\n",
    "1. __Age uncertainties__, for example comparing two records, each with an ensemble of plausible chronologies (generated, for instance, by a Bayesian age model)\n",
    "1. __Test multiplicity__ aka the \"Look Elsewhere effect\", which states that repeatedly performing the same test can result in unacceptably high type I error (accepting correlations as significant, when in fact they are not). This arises e.g. when correlating a paleoclimate record with an instrumental field, assessing significance at thounsands of grid points at once, or assessing significance within an age ensemble.\n",
    "\n",
    "Accordingly,  Pyleoclim facilitates an assessment of correlations that deals with all these cases, makes the necessary data transformations transparent to the user, and allows for one-line plot commands to visualize the results. We start by loading a few useful packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt    \n",
    "import pyleoclim as pyleo\n",
    "pyleo.set_style('web')  # set the visual style\n",
    "import numpy as np\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A case study from Crystal Cave\n",
    "In this notebook we reproduce the case of [Hu et al, 2017](http://dx.doi.org/10.1016/j.epsl.2016.11.048), particularly the example of their section 4, which illustrates several of these pitfalls at once. The example illuminates the issue of relying too strongly on correlations between a paleoclimate record and an instrumental field to interpret the record. Before we start, a disclaimer: the studies investigated in this paper are by no means isolated cases. They just happened to be cases that we knew about, and thought deserved a second look in light of more rigorous statistics. The same study could have been written by subsituting any number of other records interpreted, wholly or in part, on the basis of correlations. Accordingly, what follows should not be viewed as an indictment of a particularly study or group of authors, rather, at how easy it is by the best-intentioned scientists to get fooled by spurious correlations, and (thankfully), how easy we've made it not get similarly fooled by carrying out this analysis with `pyleoclim`. \n",
    "\n",
    "\n",
    "### The Crystal Cave record\n",
    "\n",
    "The example uses the speleothem record of [McCabe-Glynn et al , 2013](https://www.nature.com/articles/ngeo1862) from Crystal Cave, California, in the Sequoia National Forest.  Of interest to us is the $\\delta^{18}O$ record, which the authors interepret as reflecting sea-surface temperatures (SST) in the Kuroshio Extension region of the West Pacific. This is a strong claim, given that no mechanistic link is proposed,  and relies entirely on an analysis of correlations between the record and instrumental SST.  \n",
    "\n",
    "We first load and plot this record:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pyleo.Lipd('../data/Crystal.McCabe-Glynn.2013.lpd')\n",
    "cc = d.to_LipdSeries(2)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick plot to check that we have what we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = cc.plot(mute=True)\n",
    "pyleo.showfig(fig, close=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the code harvested the correct metadata from the LiPD file. If everything is in its right place, it makes it easy to exploit that information. If you're feeling more frisky, you can even ask for a whole dashboard, including a spectrum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc.dashboard(metadata=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very high resolution record, with near-annual spacing (check it), and a broadly red spectrum that exhibits a number of spectral peaks at interannual and decadal scales.\n",
    "\n",
    "\n",
    "### SST data\n",
    "\n",
    "The original paper correlated the above record against the Kaplan SST dataset.  In this notebook we instead use the [HadSST4 dataset](https://www.metoffice.gov.uk/hadobs/hadsst4/index.html),  which is more up to date, and which we first download via `wget`. (~8Mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.metoffice.gov.uk/hadobs/hadsst4/data/netcdf/HadSST.4.0.1.0_median.nc\n",
    "!mv HadSST.4.0.1.0_median.nc ../data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we load it via the excellent `xarray` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset('../data/HadSST.4.0.1.0_median.nc')\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let's only consider the Northern Hemisphere Pacific Ocean. For practicality, let's first adjust the coordinate system so that longitude is expressed from 0 to 360 degrees instead of 180W to 180E:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_rolled = ds.assign_coords(longitude=(ds.longitude % 360)).roll(longitude=(ds.dims['longitude'] // 2))\n",
    "ds_rolled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's select the needed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_sel = ds_rolled.sel(longitude=slice(120,280),latitude=slice(0,90))\n",
    "ds_sel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pitfall #1: Persistence\n",
    "\n",
    "Persistence is the tendency of many geophysical timeseries (particularly in paleoclimatology) to show some kind of memory: consecutive observations tend to resemble each other, resulting in timeseries that have fairly broad trends and low-frequency fluctuations, and comparatively little high-frequency fluctuations. \n",
    "\n",
    "This has an important consequence: the standard assumption of independence, which undergirds much of frequentist statistics, is violated in this case. In a timeseries with $n$ fully independent observations (e.g. white noise), the degrees of freedom for the variance are $DOF = n -1$  But if memory is present, this number can be drastically reduced. \n",
    "\n",
    "### Single location\n",
    "Let us look at a random location and build some intuition. First, we need to compute montly anomalies and annualize them. `xarray` makes that easy (4 lines of code), so let's look at the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = ds['tos'].sel(latitude=32.5, longitude = 142.5, method='nearest')  # 32.5N 142.5W near Kuroshio Extension\n",
    "climatology = st.groupby(\"time.month\").mean(\"time\")\n",
    "anomalies = st.groupby(\"time.month\") - climatology\n",
    "st_annual = anomalies.groupby(\"time.year\").mean(\"time\")\n",
    "f, ax = plt.subplots()\n",
    "st_annual.plot()\n",
    "pyleo.showfig(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the coverage gaps in the 1860s. This, and the fact that the Crystal Cave chronology is not uniformly spaced, would ordinarily make it challenging (at the vary least, bothersome) to compare the two series, requiring some form of interpolation or binning. Pyleoclim does all this for you under the hood.\n",
    "\n",
    "To enjoy these benefits, however, let us turn the temperature data into a _Series_ object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stts = pyleo.Series(time=st_annual.coords['year'].values,\n",
    "                    time_unit ='year CE', \n",
    "                    value=st_annual.values,\n",
    "                    value_unit = 'C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute correlations with the Crystal Cave record. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_res = stts.correlation(cc)\n",
    "print(corr_res.r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite a few things happened here. First, `pyleoclim` was smart enough to figure out a common timespan between the two records, and used linear interpolation to align the two timeseries on a common axis. \n",
    "\n",
    "The resulting correlation is $r=0.38$. Is it significant?\n",
    "\n",
    "The standard way to assess this (embedded in countless computing packages), is with a t-test using the test statistic: $$T = \\frac{r \\sqrt{n-2}}{\\sqrt{1-r^2}}$$\n",
    "\n",
    "If we plug the numbers in, we get the following:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccs = cc.slice([1854,2020])\n",
    "n = len(ccs.time)\n",
    "nu = n-2\n",
    "r = corr_res.r\n",
    "T  = r *np.sqrt(nu)/(np.sqrt(1-r**2))\n",
    "print(\"The test statistic is \"+ str(T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under standard assumptions (the data are independent and identically distributed), $T$ follows Student's $t$ distribution (the first [Guiness-soaked distribution](https://priceonomics.com/the-guinness-brewer-who-revolutionized-statistics/) in history). If we make the same assumption and use the $t$ distribution conveniently programmed for us by SciPy, we can compute the p-value (the probability to observe a test statistic at least as large as this one, under this distribution) thusly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import t\n",
    "pval = 1-t.cdf(T,nu)\n",
    "print(\"The p-value is \"+ str(pval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, using the classic test for the significance of correlations \"out of the box\", one would conclude that SST at 42N, 164E shares so much similarity with the Crystal Cave record that there are only a few chances in 10,000 that this could have happened randomly. In other words, it looks _rather_ significant. \n",
    "\n",
    "But let's take a step back. That test (which is the one that most computing packages, including Excel, will do for you out of the box), is completely inappropriate here. Why? Because it tramples over the concept of persistence with gleeful impunity. That is, it assumes that consecutive observations bear no resemblance to each other, which is true neither of the Crystal Cave nor the instrumental target. That is to say: because temperature in one year tends to resemble temperature in the previous or following year (same for $\\delta^{18}O$, the data are anything but independent. \n",
    "\n",
    "Going back to the result of the `correlation()` command, let's look at its full output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corr_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the p-value has been estimated to be 15% (`'p': 0.15`), and therefore the correlation is not deemed significant (`'signif': False`) at the 5% level (`'alpha': 0.05`).\n",
    "\n",
    "How did `pyleoclim` arrive at such a different conclusion? By not applying irrelevant assumptions, of course! To know what method was applied exactly precisely, consult the [documentation](https://pyleoclim-util.readthedocs.io/en/latest/index.html) or the docstring. \n",
    "\n",
    "**Exercise 5.1** What method does `correlation` use by default to assess significance and what are its assumptions? (hint: check out \"correlation\" on [Read The Docs](https://pyleoclim-util.readthedocs.io/en/latest/index.html), under \"The Pyleoclim user interface\"), or type `stts.correlation?` at the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 5.1** YOUR WORDS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Exercise 5.2** There are in fact 3 ways to make this determination in Pyleoclim. Try the other two in the cells below, and compare their p-values. Do they give consistent answers or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pitfall #2: Multiple testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The foregoing shows how to properly assess significance at just one location. How would we go about conducting a similar test for an entire field? Let us first try the naive approach: recursively carry out the same test as above at all grid points.  For this, we need not only to loop over grid points, but also store the p-values for later analysis. To save time, we'll use the `ttest` option for `correlation()`, knowing that it is rather approximate. Also, we need to exclude points that have too few observations.  The loop below achieves that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlon = len(ds_sel['longitude'])\n",
    "nlat = len(ds_sel['latitude']) \n",
    "pval = np.empty((nlon,nlat)) # declare array to store pvalues\n",
    "corr = np.empty_like(pval) # declare empty array of identical shape\n",
    "alpha = 0.05\n",
    "slon, slat = [], [];\n",
    "for ji in range(nlon):\n",
    "    for jj in range(nlat):     # TODO add progress bar \n",
    "        st = ds_sel['tos'][:,jj,ji]\n",
    "        climatology = st.groupby(\"time.month\").mean(\"time\")\n",
    "        anomalies = st.groupby(\"time.month\") - climatology\n",
    "        st_annual = anomalies.groupby(\"time.year\").mean(\"time\")\n",
    "        #  test if at least 100 non-NaNs\n",
    "        noNaNs = len(np.where(np.isnan(st_annual) == False)[0]) # number of valid years\n",
    "        sstvar = st_annual.var()\n",
    "        if noNaNs >= 100 and sstvar >= 0.01:\n",
    "            print(\"Computing correlation at \" + str(ds_sel.latitude[jj].values) + 'N, ' + str(ds_sel.longitude[ji].values) + 'E')\n",
    "            sttb = pyleo.Series(time=st_annual.coords['year'].values,\n",
    "                        time_unit ='year CE', \n",
    "                        value=st_annual.values,\n",
    "                        value_unit = 'C')\n",
    "            corr_res = sttb.correlation(cc, settings={'method':'ttest'})\n",
    "            pval[ji,jj] = corr_res.p\n",
    "            corr[ji,jj] = corr_res.r\n",
    "            if pval[ji,jj] < alpha:\n",
    "                slon.append(ds_sel.longitude[ji])\n",
    "                slat.append(ds_sel.latitude[jj])\n",
    "        else:  \n",
    "            pval[ji,jj] = np.nan; corr[ji,jj] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals = pval.flatten() # make the p-value array a 1D one\n",
    "pvec = pvals[pvals<1] # restrict to valid probabilities as there are a few weird values.\n",
    "nt = len(pvec)\n",
    "print(nt) # check on the final number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found 327 with enough data for a meaningful comparison, and 23 locations that pass the test. Where are they? To gain insight, let us plot the correlations and indicate (by shading) which are deemed significant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n",
    "import seaborn as sns\n",
    "\n",
    "land_color=sns.xkcd_rgb['light grey']\n",
    "ocean_color=sns.xkcd_rgb['light grey']\n",
    "\n",
    "fig = plt.figure(figsize=[8, 6])\n",
    "ax = plt.subplot(projection=ccrs.PlateCarree(central_longitude=180))\n",
    "\n",
    "# map\n",
    "ax.add_feature(cfeature.LAND, facecolor=land_color, edgecolor=land_color)\n",
    "ax.add_feature(cfeature.OCEAN, facecolor=ocean_color, edgecolor=ocean_color)\n",
    "ax.coastlines()\n",
    "\n",
    "transform = ccrs.PlateCarree()\n",
    "latlon_range = (120, 280, 0, 70)\n",
    "lon_min, lon_max, lat_min, lat_max = latlon_range\n",
    "lon_ticks = [60, 120, 180, 240, 300]\n",
    "lat_ticks = [-90, -45, 0, 45, 90]\n",
    "\n",
    "ax.set_extent(latlon_range, crs=transform)\n",
    "lon_formatter = LongitudeFormatter(zero_direction_label=False)\n",
    "lat_formatter = LatitudeFormatter()\n",
    "ax.xaxis.set_major_formatter(lon_formatter)\n",
    "ax.yaxis.set_major_formatter(lat_formatter)\n",
    "lon_ticks = np.array(lon_ticks)\n",
    "lat_ticks = np.array(lat_ticks)\n",
    "mask_lon = (lon_ticks >= lon_min) & (lon_ticks <= lon_max)\n",
    "mask_lat = (lat_ticks >= lat_min) & (lat_ticks <= lat_max)\n",
    "ax.set_xticks(lon_ticks[mask_lon], crs=transform)\n",
    "ax.set_yticks(lat_ticks[mask_lat], crs=transform)\n",
    "\n",
    "# contour\n",
    "clevs = np.linspace(-0.9, 0.9, 19)\n",
    "#corr_r, lon_r = rotate_lon(corr.T, lon)  # rotate the field to make longitude in increasing order and convert to range (0, 360)\n",
    "im = ax.contourf(ds_sel.longitude, ds_sel.latitude, corr.T, clevs, transform=transform, cmap='RdBu_r', extend='both')\n",
    "\n",
    "# significant points\n",
    "plt.scatter(x=slon, y=slat, color=\"purple\", s=3,\n",
    "            alpha=1,\n",
    "            transform=transform) \n",
    "\n",
    "\n",
    "# colorbar\n",
    "cbar_pad = 0.05\n",
    "cbar_orientation = 'vertical'\n",
    "cbar_aspect = 10\n",
    "cbar_fraction = 0.15\n",
    "cbar_shrink = 0.5\n",
    "cbar_title = 'R'\n",
    "cbar = fig.colorbar(im, ax=ax, orientation=cbar_orientation, pad=cbar_pad, aspect=cbar_aspect,\n",
    "                    fraction=cbar_fraction, shrink=cbar_shrink)\n",
    "cbar.ax.set_title(cbar_title)\n",
    "\n",
    "pyleo.showfig(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purple dots on the map are the locations of the gridpoints where the p-values fall under 5%, and they naturally correspond to the regions of highest correlations, though (and this is suspicious) they are rather randomly scattered across the domain.  \n",
    "We might be tempted to declare victory and hail them as \"significant\", but not so fast! \n",
    "Our correlation test, nifty though it is, isn't infallible. In fact, conducting tests at the 5% level (equivalently, the 95% confidence level) specifically means that we expect 5% of our tests to return spurious results, just from chance alone. We just carried out $n_t$ tests, so we expect $0.05*n_t \\approx 16 $ of those results to be bunk, right out of the gate.  Which ones can we trust?\n",
    "\n",
    "Let us rank order the p-values of all 327 tests and plot them as in Hu et al (2017), Fig 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check i/m vs. p-values\n",
    "indexm = np.arange(1,nt+1,1)\n",
    "im = 1.0*indexm / nt\n",
    "thres = 0.05*im\n",
    "pvec_s = sorted(pvec)\n",
    "smaller=[]\n",
    "small_index=[]\n",
    "larger=[]\n",
    "large_index=[]\n",
    "\n",
    "n=0\n",
    "for pp in pvec_s:\n",
    "    if pp <=0.05:\n",
    "        smaller.append(pp)\n",
    "        small_index.append(im[n])\n",
    "    else:\n",
    "        larger.append(pp)\n",
    "        large_index.append(im[n])\n",
    "    n=n+1\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "#plt.plot(im,pvec_s,'kx',markersize=1.5,label='p-values',alpha=0.3)\n",
    "plt.plot(im,thres,label='FDR threshold')\n",
    "plt.plot(small_index,smaller,'ro',markersize=1.5,label='p-values below threshold')\n",
    "plt.plot(large_index,larger,'ko',markersize=1.5,label='p-values above threshold',alpha=0.3)\n",
    "plt.axhline(y=0.05,linestyle='dashed',color='silver',label='5% threshold')\n",
    "plt.xlabel(r'$i/n_t$',fontsize=14)\n",
    "plt.ylabel(r'$p_i$',fontsize=14)\n",
    "plt.title('Correlation p-values')\n",
    "plt.legend()\n",
    "#plt.tick_params(labelsize=14)\n",
    "pyleo.showfig(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One solution to this is the False Discovery Rate (aka **FDR**), which was devised in a celebrated 1995 paper [(Benjamini & Hochberg, 1995)](https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1995.tb02031.x). The idea is to look not just for the p-values under 5% (red dots in the figure above), but for the fraction of those under the blue line, which guards against the false discoveries one expects out of repeatedly testing the same hypothesis over and over again. Luckily for you, we have this coded up in `pyleoclim` (you knew we would). One way to access it is thus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdr_res = pyleo.utils.correlation.fdr(pvec_s)\n",
    "print(fdr_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with this treatment, exactly 0 gridpoints pass the test. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5.3** redo this with other correlation methods and see if the results change. A little patience is required for the other two methods, which loop over surrogates. We suggest using a moderate number of simulations (`nsim` < 500) to do so; please check the documentation of `corr_sig` for how to reduce this number, which is 1000 by default. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Pitfall #3: Age uncertainties\n",
    "\n",
    "As if the first two pitfalls weren't enough, there is a third difficulty in this comparison: beautiful though U/Th chronologies may be, they are not perfect (do not believe people who tell you otherwise!). Such chronological uncertainties must be taken into account as well. \n",
    "\n",
    "It turns out that the LiPD file loaded above contains not only the raw U/Th dates (dig for them if you're curious), but also an ensemble of age-depth relationships derived from those dates, via the Bayesian age model [BChron](https://cran.r-project.org/web/packages/Bchron/vignettes/Bchron.html). Let us load those 1000 draws from the posterior distribution of ages and put them in a place where `pyleoclim` will be able to work with them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_ens = cc.chronEnsembleToPaleo(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to plot this ensemble. First, as a series of traces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = cc_ens.plot_traces(mute=True)\n",
    "cc.plot(color='black', ax = ax)\n",
    "pyleo.showfig(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is quite plain that any of the record's main swings can be swung back and forth by up to decades. Another way to see this is to plot various quantiles as an envelope:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = cc_ens.common_time(method='interp').plot_envelope(ylabel = cc.value_name, mute=True)\n",
    "cc.plot(color='black', ax = ax, linewidth=1)\n",
    "pyleo.showfig(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the quantiles are computed over only part of the interval covered by the original series; that is because some age models end up being more compressed or stretched out than the original, and we need a common interval to compute quantiles. In particular, notice how the base of the record could really be anywhere between 850 and 1100 AD.\n",
    "\n",
    "Now, what we'd like to do is repeat the exercise of Part 1, correlating the same SST timeseries from 32.5N 142.5W (near the Kuroshio Extension), not just with the published chronology, but this whole ensemble. Remember how we had computed things:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_res = stts.correlation(cc)\n",
    "print(corr_res.r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though of course, we could have done just the reverse, as correlation is a symmetric operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_res = cc.correlation(stts)\n",
    "print(corr_res.r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, consider the task ahead: you must iterate over all ensemble members, taking care of aligning their time axis to that of HadSST4; you must compute the correlation and establish its significance with a sensible test (say, Ebisuzaki's isospectral test, see Pitfall #1), you need watch out for test multiplicity (see Pitfall #2) _and_  you need to visualize the results in an intuitive way. Don't you wish this has all been conveniently coded for you?\n",
    "\n",
    "\n",
    "\n",
    "![Your wish](https://am23.mediaite.com/tms/cnt/uploads/2020/12/Life-Is-Good-but-It-Can-Be-Better-With-These-Max-Lord-Memes.jpg)\n",
    "\n",
    "\n",
    "YOUR WISH HAS BEEN GRANTED!!!\n",
    "\n",
    "To keep computing time manageable, let's reduce the number of isospectral simulations to 500, and correlate the ensemble and the SST series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsim = 500\n",
    "corr_Kuroshio = cc_ens.correlation(stts,settings={'nsim':nsim}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here's the best part: the `corr_Kuroshio` is an object that contains everything you want: the vector of correlations, the p-values, _and_ a method to plot them all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_Kuroshio.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several things jump out at once:\n",
    "1. the correlation histogram is relatively symmetric, meaning that age uncertainties are able to completely overturn the original correlation of about 0.31 : nearly half of the ensemble exhibits negative correlations with SST).  0.31 (9% of common variance) was exactly a contender for winning the correlation olympics, but it was at least positive.\n",
    "1. As shown in green, only a few \\% of the 500*100 correlations just computed are judged significant (note: this would change somewhat every time you run the calculation, based on the randomness of the phase randomization procedure. If reproducibility is essential, you  may also input a [random seed](https://en.wikipedia.org/wiki/Random_seed) to ensure that the same exact random sequence is used from iteration to iteration. \n",
    "1. Once we take test multiplicity into account via the False Discovery Rate (orange bar), only a fraction of a \\%  is deemed significant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5.4** redo this at other locations and see if the results change. you may consider picking a patch of the North Pacific and applying this test recursively at each location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "\n",
    "**Exercise 5.5**  To bring it all together, summarize everything that can go wrong when fishing for correlations (at sea, or on land). Can you think of other papers where correlations with paleoclimate records might have been used unwisely? (no names needed... Just keep that in mind in your own work and point people to more defensible ways of dealing with this danger, i.e. Pyleoclim!) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 5.5** YOUR WORDS HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
