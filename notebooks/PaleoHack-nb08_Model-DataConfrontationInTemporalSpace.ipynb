{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "portable-pendant",
   "metadata": {},
   "source": [
    "<img src='https://github.com/LinkedEarth/Logos/raw/master/PYLEOCLIM_logo_HORZ-01.png' width=\"800\">\n",
    "\n",
    "# 8. Model-Data Confrontation in the time domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-rebound",
   "metadata": {},
   "source": [
    "In the notebook, we demonstrate how to use `Pyleoclim` to load LiPD files, and compare proxy records with the [last millennium reanalysis (LMR)](https://cpo.noaa.gov/News/News-Article/ArtMID/6226/ArticleID/1807/Last-Millennium-Reanalysis-now-at-NOAAs-National-Centers-for-Environmental-Information-marking-major-milestone) at  proxy locales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfied-croatia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load essential packages\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "\n",
    "import pyleoclim as pyleo  # make an alias name for \"pyleoclim\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-cause",
   "metadata": {},
   "source": [
    "## Load proxy data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sized-assist",
   "metadata": {},
   "source": [
    "The proxy record we'd like to load is [this one](http://wiki.linked.earth/LPD81e53153.temperature), attached to [Tierney et al (2015)](http://dx.doi.org/10.1126/sciadv.1500682). It is an SST reconstruction based on the TEX86 proxy from two cores from the horn of Africa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-girlfriend",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pyleo.Lipd(usr_path='../data/Afr-P178-15P.Tierney.2015.lpd')\n",
    "Ocn_136 = d.to_LipdSeries(0) \n",
    "Ocn_137 = d.to_LipdSeries(2)   \n",
    "Ocn_136.label = 'Ocn_136'\n",
    "Ocn_137.label = 'Ocn_137'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-prince",
   "metadata": {},
   "source": [
    "Let's plot the two cores on the same graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breeding-tissue",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = Ocn_137.plot(mute=True)\n",
    "Ocn_136.plot(ax=ax)\n",
    "pyleo.showfig(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-debut",
   "metadata": {},
   "source": [
    "Wwe'd like to see how this compares to the [last millennium reanalysis](https://cpo.noaa.gov/News/News-Article/ArtMID/6226/ArticleID/1807/Last-Millennium-Reanalysis-now-at-NOAAs-National-Centers-for-Environmental-Information-marking-major-milestone) (LMR, [Hakim et al. 2016](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1002/2016JD024751), [Tardif et al. 2019](https://cp.copernicus.org/articles/15/1251/2019/)) at the same location. Note that LMR knows nothing of this dataset, as it (currently) only uses annually-resolved records. Thus, this exercise can serve as independent validation of LMR.  Let us first extract the geographical coordinates of the core:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cognitive-parameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "tslist = d.to_tso()\n",
    "plat = tslist[0]['geo_meanLat']\n",
    "plon = tslist[0]['geo_meanLon']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-resort",
   "metadata": {},
   "source": [
    "Now, let's move on to extract the LMR-reconstructed temperature series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-cabinet",
   "metadata": {},
   "source": [
    "## Extract LMR-reconstructed temperature series\n",
    "\n",
    "We will use the sea-surface temperature full grid ensemble [mean](https://atmos.washington.edu/%7Ehakim/lmr/LMRv2/sst_MCruns_ensemble_mean_LMRv2.1.nc) and [spread](https://atmos.washington.edu/%7Ehakim/lmr/LMRv2/sst_MCruns_ensemble_spread_LMRv2.1.nc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-explorer",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_url = 'https://atmos.washington.edu/%7Ehakim/lmr/LMRv2/sst_MCruns_ensemble_mean_LMRv2.1.nc'\n",
    "spread_url = 'https://atmos.washington.edu/%7Ehakim/lmr/LMRv2/sst_MCruns_ensemble_spread_LMRv2.1.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrative-investor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the files\n",
    "! wget $mean_url\n",
    "! wget $spread_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-truth",
   "metadata": {},
   "outputs": [],
   "source": [
    "with xr.open_dataset('sst_MCruns_ensemble_mean_LMRv2.1.nc') as ds:\n",
    "    print(ds)\n",
    "    sst_lat = ds['lat']\n",
    "    sst_lon = ds['lon']\n",
    "    sst_time = ds['time']\n",
    "    sst_mean = ds['sst']\n",
    "    \n",
    "with xr.open_dataset('sst_MCruns_ensemble_spread_LMRv2.1.nc') as ds:\n",
    "    print(ds)\n",
    "    sst_spread = ds['sst']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-wesley",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(sst_mean))  # check the shape of the LMR ensemble mean\n",
    "print(np.shape(sst_spread))  # check the shape of the LMR ensemble spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tender-decrease",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sst_time) # check the time axis from LMR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expired-privilege",
   "metadata": {},
   "source": [
    "Note that the time axis from LMR is in `cftime.DatetimeNoLeap`.\n",
    "We need to convert it to an array of floats.\n",
    "Since it's simply a list of integers from 0 to 2000, we simply define it with a numpy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-theta",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define time axis in an array of floats\n",
    "sst_time = np.arange(0, 2001)\n",
    "print(sst_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thirty-throat",
   "metadata": {},
   "source": [
    "Now we need to locate the nearest gridpoint in the LMR grid for each proxy record.\n",
    "We utilize a function below to achieve that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rough-relation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_loc(lat, lon, target_lat, target_lon, mode=None, verbose=False):\n",
    "    from scipy import spatial\n",
    "    \n",
    "    ''' Find the nearest model model point based on the given target list\n",
    "\n",
    "    Args:\n",
    "        lat, lon (array): the model latitude and longitude arrays\n",
    "        target_lat, target_lon (array): the target latitude and longitude arrays\n",
    "        mode (str):\n",
    "        + latlon: the model lat/lon is a 1-D array\n",
    "        + mesh: the model lat/lon is a 2-D array\n",
    "\n",
    "    Returns:\n",
    "        lat_ind, lon_ind (array): the indices of the found closest model sites\n",
    "\n",
    "    '''\n",
    "    if mode is None:\n",
    "        if len(np.shape(lat)) == 1:\n",
    "            mode = 'latlon'\n",
    "        elif len(np.shape(lat)) == 2:\n",
    "            mode = 'mesh'\n",
    "        else:\n",
    "            raise ValueError('ERROR: The shape of the lat/lon cannot be processed !!!')\n",
    "\n",
    "    if mode == 'latlon':\n",
    "        # model locations\n",
    "        mesh = np.meshgrid(lon, lat)\n",
    "\n",
    "        list_of_grids = list(zip(*(grid.flat for grid in mesh)))\n",
    "        model_lon, model_lat = zip(*list_of_grids)\n",
    "\n",
    "    elif mode == 'mesh':\n",
    "        model_lat = lat.flatten()\n",
    "        model_lon = lon.flatten()\n",
    "\n",
    "    elif mode == 'list':\n",
    "        model_lat = lat\n",
    "        model_lon = lon\n",
    "\n",
    "    model_locations = []\n",
    "\n",
    "    for m_lat, m_lon in zip(model_lat, model_lon):\n",
    "        model_locations.append((m_lat, m_lon))\n",
    "\n",
    "    # target locations\n",
    "    if np.size(target_lat) > 1:\n",
    "        #  target_locations_dup = list(zip(target_lat, target_lon))\n",
    "        #  target_locations = list(set(target_locations_dup))  # remove duplicated locations\n",
    "        target_locations = list(zip(target_lat, target_lon))\n",
    "        n_loc = np.shape(target_locations)[0]\n",
    "    else:\n",
    "        target_locations = [(target_lat, target_lon)]\n",
    "        n_loc = 1\n",
    "\n",
    "    lat_ind = np.zeros(n_loc, dtype=int)\n",
    "    lon_ind = np.zeros(n_loc, dtype=int)\n",
    "\n",
    "    # get the closest grid\n",
    "    for i, target_loc in (enumerate(tqdm(target_locations)) if verbose else enumerate(target_locations)):\n",
    "        X = target_loc\n",
    "        Y = model_locations\n",
    "        distance, index = spatial.KDTree(Y).query(X)\n",
    "        closest = Y[index]\n",
    "        nlon = np.shape(lon)[-1]\n",
    "\n",
    "        if mode == 'list':\n",
    "            lat_ind[i] = index % nlon\n",
    "        else:\n",
    "            lat_ind[i] = index // nlon\n",
    "        lon_ind[i] = index % nlon\n",
    "\n",
    "        #  if np.size(target_lat) > 1:\n",
    "            #  df_ind[i] = target_locations_dup.index(target_loc)\n",
    "\n",
    "    if np.size(target_lat) > 1:\n",
    "        #  return lat_ind, lon_ind, df_ind\n",
    "        return lat_ind, lon_ind\n",
    "    else:\n",
    "        return lat_ind[0], lon_ind[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breeding-benjamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lat.keys())  # check the keys we have so far"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-console",
   "metadata": {},
   "source": [
    "Now we use the `find_nearest_loc()` function to search for the nearest grid point in the LMR grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-duncan",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_idx = {}\n",
    "lon_idx = {}\n",
    "pid = 'Ocn_136'\n",
    "lat_idx[pid], lon_idx[pid] = find_nearest_loc(sst_lat, sst_lon, plat, plon)\n",
    "print(f'Target: {plat, plon}; Found: {sst_lat[lat_idx[pid]], sst_lon[lon_idx[pid]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-report",
   "metadata": {},
   "source": [
    "Now the grid point is located, we are able to define `pyleoclim.EnsembleSeries` for the LMR data.\n",
    "Note that a `pyleoclim.EnsembleSeries` is simply a list of `pyleoclim.Series`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hazardous-tissue",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dimension sizes\n",
    "nt, nEns, nlat, nlon = np.shape(sst_mean)\n",
    "\n",
    "# the dictionary to store pyleoclim.EnsembleSeries\n",
    "ms_mean = {}\n",
    "ms_spread = {}\n",
    "\n",
    "pid = 'Ocn_136'\n",
    "ts_mean_list = []\n",
    "ts_spread_list = []\n",
    "for i in range(nEns):\n",
    "    ts_mean_tmp = pyleo.Series(\n",
    "            time=sst_time,\n",
    "            value=sst_mean[:, i, lat_idx[pid], lon_idx[pid]],\n",
    "            time_name='Time',\n",
    "            value_name='LMR-temp.',\n",
    "            time_unit='AD',\n",
    "            value_unit='K',\n",
    "        )\n",
    "    ts_spread_tmp = pyleo.Series(\n",
    "            time=sst_time,\n",
    "            value=sst_spread[:, i, lat_idx[pid], lon_idx[pid]],\n",
    "            time_name='Time',\n",
    "            value_name='LMR-temp.',\n",
    "            time_unit='AD',\n",
    "            value_unit='K',\n",
    "        )\n",
    "    ts_mean_list.append(ts_mean_tmp)\n",
    "    ts_spread_list.append(ts_spread_tmp)\n",
    "    \n",
    "# define pyleoclim.EnsembleSeries\n",
    "ms_mean[pid] = pyleo.EnsembleSeries(series_list=ts_mean_list)\n",
    "ms_spread[pid] = pyleo.EnsembleSeries(series_list=ts_spread_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-justice",
   "metadata": {},
   "source": [
    "Now we let's do a quick visualization of the data with two available plotting methods:\n",
    "1. `.plot_traces()`: display several example members\n",
    "2. `.plot_envelope()`: display all members as an envelope plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-medication",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = ms_mean['Ocn_136'].plot_traces() # display several example members\n",
    "fig, ax = ms_mean['Ocn_136'].plot_envelope() # display all members as an envelope plot\n",
    "\n",
    "fig, ax = ms_spread['Ocn_136'].plot_traces() # display several example members\n",
    "fig, ax = ms_spread['Ocn_136'].plot_envelope() # display all members as an envelope plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mexican-excuse",
   "metadata": {},
   "source": [
    "Note, however, the ensemble of the means is different from the ensemble of the original reconstructed temperature series.\n",
    "To get a flavor of the original ensemble, we plot the ensemble GMST below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-pharmacology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download LMR GMST ensembles\n",
    "!wget https://atmos.washington.edu/%7Ehakim/lmr/LMRv2/gmt_MCruns_ensemble_full_LMRv2.1.nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-luxury",
   "metadata": {},
   "outputs": [],
   "source": [
    "with xr.open_dataset('gmt_MCruns_ensemble_full_LMRv2.1.nc') as ds:\n",
    "    print(ds)\n",
    "    gmt = ds['gmt'].values\n",
    "    gmt_time = np.arange(2001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-scroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exact data and define EnsembleSeries object\n",
    "ts_gmt_list = []\n",
    "nt, nMC, nM = np.shape(gmt)\n",
    "for i in range(nMC):\n",
    "    for j in range(nM):\n",
    "        ts_gmt_tmp = pyleo.Series(\n",
    "                time=gmt_time,\n",
    "                value=gmt[:,i,j],\n",
    "                time_name='Time',\n",
    "                value_name='LMR-GMST',\n",
    "                time_unit='AD',\n",
    "                value_unit='K',\n",
    "            )\n",
    "    ts_gmt_list.append(ts_gmt_tmp)\n",
    "\n",
    "ms_gmt = pyleo.EnsembleSeries(ts_gmt_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-costa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "fig, ax = ms_gmt.plot_traces()\n",
    "fig, ax = ms_gmt.plot_envelope()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elder-andrews",
   "metadata": {},
   "source": [
    "## Comparing the two reconstructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-warning",
   "metadata": {},
   "source": [
    "Now, back to the ensemble means and spreads, we are ready to perform model-data comparison.\n",
    "Since the LMR reconstruction is expressed as anomalies, we need to first calculate the anomaly series from the proxy record before the comparison. To do so, we simply call the `pyleoclim.Series.anomaly()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-separation",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = ms_mean['Ocn_136'].plot_envelope(mute=True,curve_lw=0.5,curve_clr='black',shade_clr='gray')\n",
    "Ocn_137.anomaly().plot(ax=ax, zorder=100)  # adjust zorder to reveal the curve\n",
    "Ocn_136.anomaly().plot(ax=ax, zorder=100)\n",
    "pyleo.showfig(fig)\n",
    "pyleo.closefig(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soviet-wildlife",
   "metadata": {},
   "source": [
    "We can see that the timing of industrial warming is consistent between the two cores and LMR, though pre-indsutrial variability is severely damped in LMR (because od the lack of nearby, anually resolved proxy records) particularly in the first millennium. This is because of the attrition of whatever few annually-resolved proxies there are in that part of the world, most likely coral records from the Indian Ocean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certified-guard",
   "metadata": {},
   "source": [
    "Now we calculate the correlation between the LMR median curve and the proxy record, after which we visualize the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-catering",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_ens = ms_mean['Ocn_136'].correlation(Ocn_136)\n",
    "print(corr_ens)\n",
    "\n",
    "fig, ax = corr_ens.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-earthquake",
   "metadata": {},
   "source": [
    "Not surprisingly, one finds a positive correlation, consistent among ensemble members, likely driven by the anthropogenic warming trend. More instructive would be to look at the correlation over the Common Era as a whole.\n",
    "\n",
    "**Exercise 8.1** \n",
    "How does this picture change when using the longer core (Ocn_137)?\n",
    "\n",
    "**Exercise 8.2**\n",
    "How does this picture change when using either core and the global mean surface temperature series?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-richards",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
